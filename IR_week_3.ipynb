{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_week_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashmakwana0702/Programing/blob/master/IR_week_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBqcG0taFLJk",
        "outputId": "ac9cb4a2-0812-4416-baa9-6199eca0b174"
      },
      "source": [
        "from google.colab import drive\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "import csv\n",
        "import gc\n",
        "import sys\n",
        "csv.field_size_limit(sys.maxsize) # if we don't do this, we won't be able to read whole line ( try to comment this line for action )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucof9g2EFgvs",
        "outputId": "4ce10c6f-e35a-4d5e-8272-5f36f29f14d3"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "# going to our folder\n",
        "% cd /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuutlyNbFk5g"
      },
      "source": [
        "word_doc_index = {} # took around 15/20 secs (altho this will be in our RAM now FYI)\n",
        "# Storing this in RAM will be heavy, so you have 2 ways, you can either iterate through file, or use RAM\n",
        "# Storing in RAM \n",
        "with open('./IROUTPUTS_v2/merged.txt') as f:\n",
        "  next(f) # I have one empty line\n",
        "  for line in f:\n",
        "    word, *docs = line.strip().split(' ')\n",
        "    word_doc_index[word] = tuple(docs) # tuple saves a lil bit of memory since its read only"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcWboBEeLJyg",
        "outputId": "9d51c3f5-bbc9-42a9-d609-b41bf5cecb9e"
      },
      "source": [
        "del word_doc_index\n",
        "gc.collect()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1180"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMmADxmTPYKv"
      },
      "source": [
        "doc_id_to_title = {}\n",
        "shelf_to_title = defaultdict(list)\n",
        "with open('./gutenberg_data.csv') as f:\n",
        "  next(f)\n",
        "  csv_file = csv.reader(f)\n",
        "  for line in csv_file:\n",
        "    title, author, link, id, bookshelf, text = line\n",
        "    doc_id_to_title[id] = title\n",
        "\n",
        "with open('./gutenberg_metadata.csv') as f:\n",
        "  next(f)\n",
        "  csv_file = csv.reader(f)\n",
        "  for line in csv_file:\n",
        "    title, author, link, bookshelf = line\n",
        "    shelf_to_title[bookshelf].append(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5rfoGMtRim1"
      },
      "source": [
        "# doc_id_to_title\n",
        "# shelf_to_title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qkIQjnAGILbH",
        "outputId": "7244752e-8267-4346-8076-b88f5bf46e1b"
      },
      "source": [
        "def remove_symbols(line):\n",
        "    return re.sub('[^A-Za-z0-9\\s]+', '', line).lower()\n",
        "def preprocess_word(word):\n",
        "  word = word.lower()\n",
        "  return remove_symbols(porter.stem(word))\n",
        "preprocess_word('happily')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'happili'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5M-XnYmGz5v",
        "outputId": "169a8f79-5892-4403-897c-609e4e5a541f"
      },
      "source": [
        "# so I'm just gonna make a simple query for now. we can move on to advanced at later stage\n",
        "# we also have meta data to use, we can similarly make index files for them as well\n",
        "def query(comes_with=[], doesnt_come_with=[], shelf=None):\n",
        "  # comes_with = [porter.stem(i) for i in comes_with] # as we have stemmed words inside the index\n",
        "  # doesnt_come_with = [porter.stem(i) for i in doesnt_come_with]\n",
        "  all_docs = set()\n",
        "  for word in comes_with:\n",
        "    word = preprocess_word(word)\n",
        "    docs = word_doc_index.get(word, [])\n",
        "    # below line can check if it has atleast one word, then accept the doc\n",
        "    # all_docs.update(set(docs)) # <= this line can work for unioning\n",
        "    if all_docs:\n",
        "      all_docs.intersection_update(set(docs)) # this operation will ensure that we want the doc to HAVE ALL THE WORDS\n",
        "    else:\n",
        "      all_docs.update(set(docs)) # in case we don't have anything in common\n",
        "    if not all_docs:\n",
        "      break\n",
        "    # moreover we can even break if first for first N words there are no docs in common\n",
        "  \n",
        "  negation_docs = set()\n",
        "  for word in doesnt_come_with:\n",
        "    word = preprocess_word(word)\n",
        "    docs = word_doc_index.get(word, [])\n",
        "    negation_docs.update(set(docs))\n",
        "  # removing the docs which have all terms which we dont want\n",
        "  if negation_docs:\n",
        "    all_docs.difference_update(set(docs)) # remove docs if they contain this word\n",
        "\n",
        "  doc_titles = [doc_id_to_title[i] for i in all_docs]\n",
        "  \n",
        "  if shelf:\n",
        "    shelf_docs = shelf_to_title.get(shelf, [])\n",
        "    filtered_docs = [i for i in doc_titles if i in shelf_docs]\n",
        "    return filtered_docs\n",
        "  \n",
        "  return doc_titles\n",
        "  # above step highly depends on what we are doing with word while storing\n",
        "# moreover we can do mapping on these doc_ids too\n",
        "query(comes_with=['fox'], shelf='Animal')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Fifty Years a Hunter and Trapper',\n",
              " 'Antarctic Penguins: A Study of Their Social Habits',\n",
              " 'Wild Animals at Home',\n",
              " 'Natural History of the Mammalia of India and Ceylon',\n",
              " 'Deadfalls and Snares',\n",
              " 'The Bird Book',\n",
              " 'Animals of the Past',\n",
              " \"Wolf and Coyote Trapping: An Up-to-Date Wolf Hunter's Guide\",\n",
              " 'Birds, Illustrated',\n",
              " 'Hunting in Many Lands: The Book of the Boone and Crockett Club',\n",
              " 'Birds from North Borneo',\n",
              " 'Birds and Man',\n",
              " 'What Bird is That?',\n",
              " 'The Extermination of the American Bison',\n",
              " 'Zoological Mythology; or, The Legends of Animals, Volume 1 (of 2)',\n",
              " 'Fox Trapping: A Book of Instruction Telling How to Trap, Snare, Poison and Shoot',\n",
              " 'Science of Trapping',\n",
              " 'Our Vanishing Wild Life: Its Extermination and Preservation',\n",
              " 'Sketches of the Natural History of Ceylon',\n",
              " 'Artistic Anatomy of Animals',\n",
              " 'Butterflies and Moths (British)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}